# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15F05ouqAiieJ276T3Q_536TIYm-4FeZ0
"""

!pip install numpy pandas scikit-learn matplotlib seaborn

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml

from google.colab import drive
drive.mount('/content/drive')

# Load a dataset from your drive (example with a CSV file)
import pandas as pd
file_path = '/content/HousingData.csv'  # Change this to your file path
data = pd.read_csv(file_path)
data.head()



# Load the Boston dataset from OpenML
boston = fetch_openml(name="boston", version=1, as_frame=True)
data = boston.frame  # get it as a DataFrame

#Display the first few rows of the dataset
print(data.head())

# Exploratory Data Analysis (EDA)
# Checking for missing values
print(data.isnull().sum())

# Check the column names to ensure 'MEDV' is the target variable
print(data.columns)

# Visualize the distribution of the target variable 'MEDV' (Median House Prices)
sns.histplot(data['MEDV'], bins=30, kde=True)
plt.title('Distribution of House Prices')
plt.xlabel('Price in $1000s')
plt.ylabel('Frequency')
plt.show()

# Correlation Heatmap to understand relationships
plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Define features and target
X = data.drop("MEDV", axis=1)
y = data["MEDV"]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data Preprocessing: Standardizing features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model Selection: Training Linear Regression and Random Forest models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, random_state=42)
}

# Scatter plot of actual vs. predicted prices
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_final_pred, alpha=0.7)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs. Predicted House Prices")
plt.show()

# Residual Plot
residuals = y_test - y_final_pred
sns.histplot(residuals, bins=30, kde=True)
plt.title("Residuals Distribution")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.show()

#comparing this linear regression and Random forest to  XGBoost and LightGBM.

#comparing this linear regression and Random forest to  XGBoost and LightGBM.



!pip install xgboost lightgbm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import lightgbm as lgb

!pip install openml
import openml # Import the openml library

#Load the Boston housing dataset from OpenML
boston = openml.datasets.get_dataset(531)
df, *_ = boston.get_data()

import openml
# Load the Boston housing dataset from OpenML
boston = openml.datasets.get_dataset(531)
df, *_ = boston.get_data()

# View the dataset
print(df.head())

# Split the data into features and target
X = df.drop('MEDV', axis=1)  # Features
y = df['MEDV']  # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (Standardize features)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# XGBoost Model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

# LightGBM Model
lgb_model = lgb.LGBMRegressor()
lgb_model.fit(X_train, y_train)
lgb_preds = lgb_model.predict(X_test)

# Evaluation: Mean Squared Error, Mean Absolute Error, R-squared
def evaluate_model(model_preds, y_test):
    mse = mean_squared_error(y_test, model_preds)
    mae = mean_absolute_error(y_test, model_preds)
    r2 = r2_score(y_test, model_preds)
    return mse, mae, r2

# XGBoost Evaluation
xgb_mse, xgb_mae, xgb_r2 = evaluate_model(xgb_preds, y_test)
print(f'XGBoost Results:\nMSE: {xgb_mse}\nMAE: {xgb_mae}\nR-squared: {xgb_r2}')

# LightGBM Evaluation
lgb_mse, lgb_mae, lgb_r2 = evaluate_model(lgb_preds, y_test)
print(f'LightGBM Results:\nMSE: {lgb_mse}\nMAE: {lgb_mae}\nR-squared: {lgb_r2}')

# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import lightgbm as lgb
import openml

# Load the Boston housing dataset from OpenML
boston = openml.datasets.get_dataset(531)
df, *_ = boston.get_data()

# View the dataset
print(df.head())

# Split the data into features and target
X = df.drop('MEDV', axis=1)  # Features
y = df['MEDV']  # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (Standardize features)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(),
    "XGBoost": xgb.XGBRegressor(),
    "LightGBM": lgb.LGBMRegressor()
}

# Dictionary to store evaluation results
results = {}

# Train and evaluate each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test data
    predictions = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, predictions)
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)

    # Store results
    results[model_name] = {"MSE": mse, "MAE": mae, "R2": r2}

# Print results
for model_name, metrics in results.items():
    print(f"\n{model_name} Results:")
    print(f"Mean Squared Error (MSE): {metrics['MSE']}")
    print(f"Mean Absolute Error (MAE): {metrics['MAE']}")
    print(f"R-squared (RÂ²): {metrics['R2']}")

# Identify the best model based on R2 score
best_model = max(results, key=lambda x: results[x]["R2"])
print(f"\nThe best model based on R-squared is: {best_model}")